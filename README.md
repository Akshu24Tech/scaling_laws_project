# ðŸ“ Scaling Laws Project: Discovering the Rules of AI Building

This project is a hands-on implementation of the core ideas from the groundbreaking paper
**â€œScaling Laws for Neural Language Modelsâ€ by Kaplan et al. (2020)**.

## ðŸ§  The Big Idea: Learning the Rules of Building

Think of it like this:
The researchers at OpenAI built a giant, city-sized LEGO skyscraper. We donâ€™t have enough bricks or space for that.

Instead, weâ€™re building a series of **smaller LEGO models**, from a tiny house to a tall tower, to see if we can uncover the same architectural rules they did.

Our goal isn't to build the *biggest* model â€” it's to understand the **rules of building**.
We aim to **personally verify the paperâ€™s key insight**:

> A model's performance improves **predictably** as you give it more "bricks" (increase its size).

---

## ðŸ”¬ What We're Testing

We focus on verifying the **first and most fundamental scaling law** from the paper:

> **Performance vs. Model Size (N)**
> As the number of parameters (**N**) increases, the loss decreases in a **predictable, power-law** fashion â€” visible as a **straight line on a log-log plot**.

---

## ðŸ—‚ï¸ Project Structure

```
scaling_laws_project/
â”œâ”€â”€ configs/              # Configs for different model sizes (our blueprints)
â”‚   â”œâ”€â”€ small_model.yaml
â”‚   â””â”€â”€ medium_model.yaml
â”œâ”€â”€ data/                 # Script to download the dataset (our instruction manuals)
â”‚   â””â”€â”€ prepare_data.py
â”œâ”€â”€ models/               # Transformer model architecture (our LEGO bricks)
â”‚   â””â”€â”€ transformer.py
â”œâ”€â”€ analysis/             # Notebook to visualize and analyze the results
â”‚   â””â”€â”€ plot_results.ipynb
â”œâ”€â”€ train.py              # Main script to train all models
â””â”€â”€ requirements.txt      # Python dependencies
```

---

## ðŸš€ Getting Started

### 1. ðŸ“¦ Install Requirements

Install the necessary libraries:

```bash
pip install -r requirements.txt
```

---

### 2. ðŸ“š Prepare the Dataset

The `train.py` script will **automatically download and prepare the WikiText-2 dataset** the first time you run it.
No manual steps needed.

---

### 3. ðŸ§ª Run the Experiments

Start training the models:

```bash
python train.py
```

This will:

* Train all models defined in the `configs/` directory.
* Save results to `analysis/results.csv`.

> âš ï¸ **Note:** This process may take time depending on your hardware. Be patient!

---

## ðŸ“Š Analyzing the Results

1. Open the notebook:

```bash
analysis/plot_results.ipynb
```

2. Run all cells to generate a **log-log plot** of:

> `Validation Loss` vs. `Model Parameters`

---

## âœ… What to Expect

If successful, youâ€™ll see:

* A **downward-sloping straight line** on the log-log plot.
* This confirms the **performance improves predictably** with model size â€” matching the original findings.

ðŸŽ‰ Congratulations â€” you've just rediscovered one of the **core laws of AI scaling**!

---

## ðŸ“„ Reference

* **Paper**: [Scaling Laws for Neural Language Models (Kaplan et al., 2020)](https://arxiv.org/abs/2001.08361)
